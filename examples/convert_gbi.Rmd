---
title: "Convert group-by-individual data to long format"
output:
  rmarkdown::github_document
---

This is a short notebook describing how to convert group-by-individual matrices to the long format used by BISON, and a brief demonstration of how the model can be fitted.

*WARNING: Group-by-individual matrices with large numbers of individuals or observations will generate very large dataframes that may be difficult to work with and fit. This is an area for future development we are currently working on. In the meantime, aggregated models and/or variational Bayes might be useful (see the paper for discussion of aggregated models and the Stan documentation for discussion of variational Bayes).*

```{r}
library(dplyr)
library(rstan)
library(INLA)
```

## Simulate some data

For the example, simulate some random GBI data.

```{r}
obs <- t(sapply(1:50, function(x) rbinom(8, 1, runif(1, min=0.2, max=0.4))))
obs
```

## Convert to long format

This code loops through each observation period (row in the GBI matrix) and for each present individual iterates through all possible social events that could have occurred in that observation period. Each social event that could have theoretically been observed is included as a row in the final dataframe, but only social events that were observed are assigned `social_event=1`, the others being assigned `social_event=0`. The ID of the observation period (the row id in the GBI matrix) is also included so that pseudo-replication of data points can be accounted for by including observation period as an effect in the model.

```{r}
df <- data.frame(node_1=numeric(), node_2=numeric(), social_event=numeric(), obs_id=numeric())
for (obs_id in 1:nrow(obs)) {
  for (i in which(obs[obs_id, ] == 1)) {
    for (j in 1:ncol(obs)) {
      if (i != j) {
        # Swap i and j if necessary to make sure node_1 < node_2, not essential but makes things a bit easier when assigning dyad IDs.
        if (i < j) {
          node_1 <- i
          node_2 <- j
        } else {
          node_1 <- j
          node_2 <- i
        }
        df[nrow(df) + 1, ] <- list(node_1=node_1, node_2=node_2, social_event=(obs[obs_id, i] == obs[obs_id, j]), obs_id=obs_id)
      }
    }
  }
}
head(df)
```

## Prepare dataframe for Stan model

Add dyad IDs to the dataframe and convert observation (or group) IDs to integers for indexing.

```{r}
df <- df %>%
  group_by(node_1, node_2) %>%
  mutate(dyad_id=cur_group_id()) %>%
  mutate(obs_id=as.integer(obs_id))
```

Prepare data list for model.

```{r}
model_data <- list(
  N=nrow(df), # Number of observations
  M=length(unique(df$dyad_id)), # Number of dyads
  G=nrow(obs), # Number of groupings
  dyad_ids=df$dyad_id, # Vector of dyad IDs corresponding to each observation
  group_ids=df$obs_id, # Vector of group IDs corresponding to each observation
  event=df$social_event # Vector of binary values (0/1, presence/absence) corresponding to each observation
)
```

Compile and run model.

```{r, warning=FALSE, message=FALSE, results='hide'}
model <- stan_model("../models/group_model.stan")
```

```{r}
fit <- sampling(model, model_data, cores=4)
```

Do a quick visualisation of the parameter values.

```{r}
plot(fit, pars="logit_p")
```
The rest of the analysis can proceed in the same way as the other analyses.

```{r}
df_inla <- df
df_inla$dyad_id <- as.factor(df$dyad_id)
df_inla$obs_id <- as.factor(df$obs_id)
prior.fixed <- list(mean=0, prec=1)
prior.random <- list(prec=list(prior="normal", param=c(0, 1)))
fit_inla <- inla(social_event ~ 0 + dyad_id + f(obs_id, model="iid", hyper=prior.random), 
                 family="binomial", 
                 data=df_inla,
                 control.fixed=prior.fixed,
                 control.compute=list(config = TRUE)
)
summary(fit_inla)$fixed
```

```{r}
est_inla <- summary(fit_inla)$fixed[1:28, 1]
est_stan <- summary(fit)$summary[1:28, 1]

plot(est_stan, est_inla)
```

```{r}
# How many samples to draw from posteriors. 4000 to match Stan, but samples are really cheap here so no reason not to increase this.
num_samples <- 4000
logit_p_samples <- matrix(0, nrow=num_samples, ncol=model_data$M)

param_names <- names(fit_inla$marginals.fixed)
for (i in 1:model_data$M) {
  param_zmarg <- inla.zmarginal(fit_inla$marginals.fixed[[param_names[i]]], silent=TRUE)
  
  mu <- param_zmarg[[1]]
  sigma <- param_zmarg[[2]]
  
  logit_p_samples[, i] <- rnorm(num_samples, mu, sigma)
}
logit_p_samples
```

```{r}
p_samples <- plogis(logit_p_samples)
adj_tensor <- array(0, c(8, 8, num_samples))
for (dyad_id in 1:model_data$M) {
  dyad_row <- df_agg[df_agg$dyad_id == dyad_id, ]
  adj_tensor[dyad_row$node_1_id, dyad_row$node_2_id, ] <- p_samples[, dyad_id]
}
adj_tensor[, , 1] # Print the first sample of the posterior distribution over adjacency matrices
```

```{r}
# Calculate lower, median, and upper quantiles of edge weights. Lower and upper give credible intervals.
adj_quantiles <- apply(adj_tensor_transformed, c(1, 2), function(x) quantile(x, probs=c(0.025, 0.5, 0.975)))
adj_lower <- adj_quantiles[1, , ]
adj_mid <- adj_quantiles[2, , ]
adj_upper <- adj_quantiles[3, , ]

# Calculate width of credible intervals.
adj_range <- adj_upper - adj_lower
adj_range[is.nan(adj_range)] <- 0

# Generate two igraph objects, one form the median and one from the standardised width.
g_mid <- graph_from_adjacency_matrix(adj_mid, mode="undirected", weighted=TRUE)
g_range <- graph_from_adjacency_matrix(adj_range, mode="undirected", weighted=TRUE)

# Plot the median graph first and then the standardised width graph to show uncertainty over edges.
coords <- igraph::layout_nicely(g_mid)
plot(g_mid, edge.width=3 * E(g_mid)$weight, edge.color="black",  layout=coords)
plot(g_mid, edge.width=20 * E(g_range)$weight, edge.color=rgb(0, 0, 0, 0.25), 
     vertex.label=c("Rey", "Leia", "Obi-Wan", "Luke", "C-3PO", "BB-8", "R2-D2", "D-O"), 
     vertex.label.dist=4, vertex.label.color="black", layout=coords, add=TRUE)
```
