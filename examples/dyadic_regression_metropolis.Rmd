---
title: "Dyadic Regression with Metropolis"
output: html_notebook
---

Dyadic regression is a regression analysis often used in social network analysis to determine factors that may be associated with edge weight, for example, age or sex. In this notebook we will use edge weight posteriors from a previously-run edge weight model from binary data to conduct a dyadic regression of edge weight against dyad type. In our toy example, dyad type will be one of `lifeform-lifeform`, `lifeform-droid`, and `droid-droid`. For this example we will be using a Metropolis-Hastings sampler to maintain uncertainty over the data. Alternatively, an approximation of uncertainty over edge weights can be used instead, and is shown in `dyadic_regression_stan.Rmd`.

# Setup and Loading the edge weights

Before we start, we'll need to load required libraries and the metropolis sampler from the custom R file `sampler.R`. 

```{r}
source("../scripts/sampler.R")
```

We'll load edge weight posteriors in from a previous run model using the `readRDS()` function. These data were generated by the `ewm_binary.Rmd` example in the Github repository.

```{r}
data <- readRDS("../example_data/binary.RData")
df <- data$df
df_agg <- data$df_agg
logit_p_samples <- data$logit_p_samples
adj_tensor <- data$adj_tensor
```

# Preparing the data

Now that the edge weights are loaded, we need to prepare the data for fitting the dyadic regression model. This involves 1) preparing a response matrix of edge weights describing the distribution of edge weights for each dyad, and 2) building a predictor matrix indicating which dyad type each dyad belongs to (1, 2, and 3 corresponding to ll, ld, and dd), and which two nodes make up the dyad (1 through 8 in this case).

```{r}
# Assign dyad types (ll, ld, dd)
dyad_types <- matrix(0, 8, 8)
dyad_types[1:4, 1:4] <- "ll"; dyad_types[5:8, 5:8] <- "dd"; dyad_types[1:4, 5:8] <- "ld"; dyad_types[5:8, 1:4] <- "ld"
dyad_types <- dyad_types[upper.tri(dyad_types)]

# Matrix where each the i is repeated along each column. Necessary in the next steps.
node_ids_i <- matrix(rep(1:8, 8), 8)

# Build standardised edge matrix with 4000 rows (for each posterior sample) and 28 columns (for each edge).
edge_matrix <- t(sapply(1:dim(adj_tensor)[3], function(i) adj_tensor[, , i][upper.tri(adj_tensor[, , i])]))
edge_matrix_std <- (edge_matrix - apply(edge_matrix, 1, mean))/apply(edge_matrix, 1, sd)

# Build predictor matrix with 28 rows and 3 columns, where a 1 in each row denotes to which of ll, ld, or dd the edge belongs.
predictor_matrix <- matrix(0, nrow=28, ncol=3)
colnames(predictor_matrix) <- c("dyad_type", "node_i_id", "node_j_id")
predictor_matrix[dyad_types == "ll", 1] <- 1
predictor_matrix[dyad_types == "dd", 1] <- 2
predictor_matrix[dyad_types == "ld", 1] <- 3
predictor_matrix[, 2] <- node_ids_i[upper.tri(node_ids_i)]
predictor_matrix[, 3] <- t(node_ids_i)[upper.tri(node_ids_i)]
predictor_matrix
```

# Defining the model

Now everything else is in place, it's time to define the model. As in Stan, the model will be defined by its log-likelihood function, but since we're using a custom Metropolis sampler directly in R, the log-likelihood function will need to be written directly in R. Writing likelihood functions is beyond the scope of this tutorial, but a good resource on the topic can be found here: https://www.ime.unicamp.br/~cnaber/optim_1.pdf.

The dyadic regression model we'll be using will predict the standardised edge weight (Z-score) using a Gaussian family model where dyad type is the main effect, and multi-membership terms are included as random effects to account for non-independence between edges due to nodes. Priors are set relatively narrow to improve model fit for the purposes of this example, but in any real analysis they should be determined by domain knowledge and predictive checks.

```{r}
loglik_dyadreg <- function(params, Y, X, index) {
  ### Define parameters ###
  intercept <- params[1]
  beta_dyadtype <- params[2:4]
  sigma <- exp(params[5]) # Exponential keeps underlying value unconstrained, which is much easier for the sampler.
  sigma_mm <- exp(params[6])
  mm <- params[7:14]
  
  ### Sample data according to index ###
  y <- Y[index %% dim(Y)[1] + 1, ]
  
  ### Define model ###
  
  target <- 0
  
  ## Linear predictor ##
  # y ~ normal(b_intercept + b_dyadtype + mm[i] + mm[j], sigma)
  target <- target + sum(dnorm(y, mean=intercept + beta_dyadtype[X[, 1]] + mm[X[, 2]] + mm[X[, 3]], sd=sigma, log=TRUE)) # Main model
  # b_intercept ~ normal(0, 0.5)
  target <- target + dnorm(intercept, mean=0, sd=0.5, log=TRUE)
  # b_dyadtype ~ normal(0, 0.5)
  target <- target + sum(dnorm(beta_dyadtype, mean=0, sd=0.5, log=TRUE))
  # sigma ~ exponential(1)
  target <- target + dexp(sigma, rate=0.5, log=TRUE)
  
  ## Node multimembership terms ##
  # mm ~ normal(0, sigma_mm)
  target <- target + sum(dnorm(mm, mean=0, sd=sigma_mm, log=TRUE))
  # sigma_mm ~ exponential(1)
  target <- target + dexp(sigma_mm, rate=0.5, log=TRUE)
  
  return(target)
}

# Create the `target` function that evaluates the log-likelihood on the dataset.
target <- function(params, index) loglik_dyadreg(params, edge_matrix_std, predictor_matrix, index)
```

We now have a function `target(params, index)` that gives the log-likelihood of a set of parameters `params` given the data, for a particular sample of the posterior `index`. Let's make sure it works on an initial set of parameters all set to zero, for an arbitrary index of the data. Note that the standard deviation parameters are transformed by an exponential in the log-likelihood function, so the sampler will treat them on the log-scale, meaning log(sigma) = 0 is equivalent to sigma = 1.

```{r}
target(rep(0, 14), 1)
```

The function has evaluated to a real number, so everything appears to be working okay so far. Now we can use the function `metropolis` from `sampler.R` to fit the model using the provided target function, an initial set of parameters (again, all zeros), and some additional MCMC options. Once the sampler has run, we will print out the top few rows of the chains.

```{r}
chain <- metropolis(target, rep(0, 14), iterations=100000, thin=100, refresh=10000)
colnames(chain) <- c("Intercept", "b_ll", "b_ld", "b_dd", "log(sigma)", "log(mm_sigma)", sapply(1:8, function(i) paste0("mm_", i)))
head(chain)
```

The acceptance rate is around 0.23, which is the target acceptance rate for this sampler. Deviances from 0.23 could indicate sampling issues, but the converse is not true, so acceptance rate isn't an ideal diagnostic tool. Instead it's best to inspect the traceplots, which can be done using the following code:

```{r}
par(mfrow=c(4, 4), mar=c(1,1,1,1))
for (i in 1:14) {
  plot(chain[, i], type="l")
}
```

**Note: In our runs of the code, these traces usually look good. But the stochastic nature of MCMC and the experimental sampler mean that sometimes the chains may not behave well. If the chains in this notebook have not converged, this is simply an artefact of this stochasticity.**

At this point it's a good idea to do some diagnostic checks, such as predictive checks or residual plots. These are covered separately in the Github repository, so diagnostic checks aren't shown here, but should always be carried out.

# Interpreting the model

Pretending we've now carried out any diagnostics we think are appropriate, it's finally time to answer the scientific questions that led us to conduct the analysis. We can start off by calculating the 89% credible intervals of the model parameters. This can be done using the following code:

```{r}
summary_matrix <- t(apply(chain, 2, function(x) quantile(x, probs=c(0.5, 0.055, 0.945))))
summary_matrix <- round(summary_matrix, 2)
summary_matrix
```

At this point it becomes clear that the regression we've conducted is not the same as what might be expected from standard frequentist regressions, where categories are interpreted relative to a reference category. In many cases there's no desire for a reference category, and its presence can necessitate various forms of post-hoc analysis. This is necessitated by the mathematics of most frequentist models, but this is not the case for Bayesian models. Instead, we can use *contrasts* to calculate the magnitude of differences between categories of interest. Contrasts are easily calculated as the statistic of interest from the posteriors of the model. Namely, if we're interest in whether lifeform-lifeform edges are stronger than droid-droid edges, we simply compute the difference in posteriors between `b_ll` and `b_dd`. This can be done using the following code:

```{r}
b_diff <- chain[, "b_ll"] - chain[, "b_dd"]
b_diff_summary <- round(quantile(b_diff, probs=c(0.5, 0.055, 0.945)), 2)
b_diff_summary
```

This gives us an estimate that lifeform-lifeform edges are generally around ~1.7 standard deviations stronger than droid-droid edges, with an 89% credible interval of around ~(1.2, 2.2).

# Conclusion

This notebook has given a brief overview of how to fit and interpret a dyadic regression model using the Metropolis-Hastings sampler in BISoN. We also have a guide on how to conduct the same type of analysis in Stan using a Gaussian approximation of edge weights, which should yield similar results and may be less prone to model fitting issues.
